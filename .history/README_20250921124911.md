# MCP Architecture

A demonstration of a robust and scalable architecture for implementing the Model Context Protocol (MCP) in agentic AI workflows.

## Overview

This project provides a practical implementation of the Model Context Protocol (MCP). It is designed to serve as a blueprint for developers looking to build agentic AI systems that can securely and efficiently interact with external tools, data sources, and APIs. The architecture separates concerns between the MCP Host, Client, and Server, allowing for a modular and scalable solution.

## Features

- **MCP Host:** [e.g., A Streamlit/FastAPI application that serves as the user interface and orchestrator.]
- **MCP Client:** [e.g., Manages the discovery of and communication with MCP servers.]
- **MCP Server(s):** Includes example servers for common tasks:
  - `File System Server`: Allows the AI to read/write to the local file system.
  - `Database Server`: Provides an interface for querying a SQL/NoSQL database.
  - `API Server`: Connects to a third-party API (e.g., a weather API, a CRM).
- **Standardized Communication:** Uses JSON-RPC 2.0 for reliable data exchange between components.
- **Modularity:** Easily extend the system by adding new MCP servers for different tools or services.

## Architecture Diagram

[If you have an architecture diagram, you can embed it here. If not, you can describe the flow.]

This project follows a classic MCP client-server model:

1.  **User Interaction:** The user sends a prompt to the **MCP Host** (e.g., via a web interface).
2.  **LLM Processing:** The Host forwards the prompt to the Large Language Model (LLM).
3.  **Tool Discovery:** The LLM determines it needs to use a tool. The **MCP Client** within the host discovers available **MCP Servers**.
4.  **Action Request:** The Client sends a formatted request to the appropriate Server (e.g., a request to read a file).
5.  **Execution:** The Server executes the action and returns a result.
6.  **Response Generation:** The Client relays the result back to the LLM, which uses the information to generate a final response for the user.

## Technologies Used

- **Backend:** [e.g., Python, FastAPI]
- **Frontend:** [e.g., Streamlit, HTML/CSS]
- **LLM:** [e.g., OpenAI, Anthropic Claude, Google Gemini]
- **Database:** [e.g., SQLite, PostgreSQL]
- **Key Libraries:** [e.g., `jsonrpc`, `httpx`, `sqlalchemy`]

## Getting Started

### Prerequisites

- Python 3.9+
- An API key for [Your chosen LLM provider]
- [Any other prerequisites like Docker, Node.js, etc.]

### Installation

1.  **Clone the repository:**
    ```
    git clone https://github.com/nileshgode/MCP_Architecture.git
    cd MCP_Architecture
    ```

2.  **Create and activate a virtual environment:**
    ```
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install the dependencies:**
    ```
    pip install -r requirements.txt
    ```

4.  **Set up environment variables:**
    Create a `.env` file in the root directory and add your API keys:
    ```
    OPENAI_API_KEY="your_api_key_here"
    DATABASE_URL="your_database_connection_string"
    ```

### Running the Application

1.  **Start the MCP Server(s):**
    [Provide commands to run each of your servers. For example:]
    ```
    python servers/file_system_server.py
    ```

2.  **Start the MCP Host application:**
    ```
    streamlit run app.py
    ```

3.  Open your browser and navigate to `http://localhost:8501`.

## Usage

Once the application is running, you can interact with the agent through the web interface. Try prompts that require tool use, such as:

- "Read the file named `data.csv` and summarize its contents."
- "What is the current weather in London?"
- "Find the user with the email 'example@example.com' in the database."

## How to Contribute

Contributions are welcome! If you would like to contribute, please follow these steps:

1.  Fork the repository.
2.  Create a new branch (`git checkout -b feature/YourFeature`).
3.  Make your changes.
4.  Commit your changes (`git commit -m 'Add some feature'`).
5.  Push to the branch (`git push origin feature/YourFeature`).
6.  Open a Pull Request.

## License

This project is licensed under the [MIT License](LICENSE).
